\section{Evaluation}
\label{sec:evaluation}

To evaluate the performance of TreeNose in different programming language environments,
we implemented TreeNose in 9 large open-source projects with at least 10,000 lines of code written in Java, JavaScript, or
Python. After ensuring the performance of TreeNose, we utilized TreeNose to analyze the structural patterns
of the selected code smells like distribution in the different programming languages. Therefore, we implemented TreeNose in up to 15 open-source projects, including
ones written in the combination of the selected languages. We designed the following research questions:

{\it RQ1: How does TreeNose perform in different languages?}
This RQ aims to evaluate the accuracy of TreeNose in detecting code smells in different programming languages. 
We selected 3 language-specific code smell detection tools to compare TreeNose's detection accuracy with theirs.

{\it RQ2: How do code smells distribute in various languages?}
By investigating the distribution of code smells in different programming languages, this RQ is designed to understand 
languages' tendencies to contain specific code smells.

{\it RQ3: How often do code smells occur in various languages?}
This RQ intends to evaluate the prevalence of the selected code smells in different programming languages.

\subsection{Subjects}
\label{sec:subjects}

To answer the RQs, the subjects must stand for the real-world software projects in their respective 
programming languages. They should be actively used and well-maintained, allowing the experiments to filter out 
the noise in softwares and focus on the code smells on programming language level. To achieve this goal, the subjects in the 
experiments are 1. have commits on main branch in the last year, 2. have at least 10,000 lines of code, and 3. have more than 1,000 stars on GitHub.


% You should describe details of the subjects that you used in your study,
% summarised in a table, such as the example. The information in the file ``{\tt
% figures/plain-figure.tex}'' describes how to format a figure, included here as
% Always reference tables like you would reference sections/figures, with a
% capital "T" for table a non-breaking space (the ~ character) between "Table"
% and "\ref" to ensure the word and the number are not split over two lines.
% Table~\ref{tab:plain-table}.

\subsection{Methodology}
\label{sec:methodology}

To answer the RQ1, we compared TreeNose with 3 language-specific code smell detection tools, namely Pysmell 
(Python), Jscent (JavaScript), and DesigniteJava (Java).
All 3 code smell detectors are language-specific AST-based code smell detection tools 
that are open-source, where Pysmell was proven to achieve 97.7\% precision in detecting code smells in Python.
Those detectors support most code smells detected by TreeNose, allowing us to implement them in the same sets of projects with TreeNose to compare their performances.
To quantify the performance of TreeNose and compare it with the other tools, we adapted precision and recall, which are defined as follows:


\begin{align*}
    Precision = \frac{TP}{TP + FP} & Recall = \frac{TP}{TP + FN}
    \end{align*}


\begin{equation}
    Precision = \frac{True Positive}{True Positive + False Positive}
\end{equation}

\begin{equation}
    Recall = \frac{True Positive}{True Positive + False Negative}
\end{equation}

To represent both precision and recall in a single metric, we calculated the F 1 score, the harmonic mean of precision and recall, which is defined as follows:

\begin{equation}
    F1 = 2 * \frac{Precision * Recall}{Precision + Recall}
\end{equation}

To our best knowledge, there is no annotated dataset for code smells in our selected programming languages, we hence had to design an annotation study by ourselves.
First, we implemented both TreeNose and the other tools with their default thresholds to analyze the same sets of projects.
We, furthermore, conducted such a study with 2 internal developers to calculate the F1 score of TreeNose. Specifically, we built two spreadsheets for the study. 
One for the code smell reports generated by TreeNose, and another for the code smell reports generated by the other tools. 
With the code smell reports generated by the other tools, we randomly selected 5 samples out of each code smell in each programming language system. 
, and then put those samples in the spreadsheet with their related source code and a column for the detection decision of TreeNose.
After that, we asked the developers to independently inspect the samples and determine whether the samples are code smells or not. 
With the independent inspection results, we built a consensus after the developers discussed the samples they disagreed on.
Here, we defined the True Positive of TreeNose as the number of code smells that are detected by both TreeNose and the developers among the samples selected from the other tools, 
the False Positive as the number of code smells that are detected by TreeNose but not by the developers, and the False Negative as the number of code smells that are detected by the developers but not by TreeNose.
With those metrics, we calculated the F1 score of TreeNose. Then, we conducted the same study with the samples from TreeNose to calculate the F1 score of the other tools. 
Finally, we compared the F1 scores of TreeNose and the other tools to evaluate their performances. It's noteworthy that not all code smells detected by TreeNose are detected by the other tools. 
For those code smells, we skipped them in the calculation of the F1 score of TreeNose because there is no detection results from the other tools,
and tagged them as not detected in the calculation of the F1 score of the other tools. We made this decision because the other tools silently failed to detect those code smells.


To answer the RQ2 and RQ3, we implemented TreeNose in up to 20 open-source projects, including the subjects in RQ1.
 We analyzed software projects written in 3 selected programming languages, and ones in the combination of the selected programming languages. 
 We implemented TreeNose in the projects and analyzed the occurrence of each code smell in the reports in each code smell in the projects.
Analyzing the code smell reports of TreeNose in the projects helps to understand the distribution 
and prevalence of the selected code smells in different programming languages, and furthermore the tendencies of code smells on each programming language.
To minimize the bias introduced by the different sizes of the projects in the RQ2 and the RQ3, we calculate the metrics of each code smell in individual 
projects and take the average of the metrics in those projects to calculate the language-level metrics. 

In the experiment of the RQ2, we calculate the percentage of each code smell out of all the selected code smells as follows:

\begin{equation}
    Percentage the code smell in the language = \frac{\sum{\frac{Number of the occurrence of the code smell}{Total number of the code smells}}}{Number of the projects in the language}
\end{equation}

As for the RQ3, 
we count the occurrence of each code smell per 1,000 lines of code in the projects,
and then take the average of the occurrences in each project. Finally by taking the average of the occurrences in the projects in the language, 
we calculate the prevalence of the code smell in the language as follows:

\begin{equation}
    Prevalence of the code smell in the language = \frac{\sum{\frac{Number of the occurrence of the code smell}{Total number of the lines of code}}}{Number of the projects in the language}
\end{equation}
\subsection{Threats to Validity}
\label{sec:threats-to-validity}

The definition of code smell, code that is hard to comprehend or maintain, subjective. Therefore, one of concerns in this research is on the
definitions of the selected code smells. We noted that different code smell detection tools may utilize 
different metrics and thresholds to detect the same code smell. Therefore, the metrics of TreeNose may not be consistent with the other tools.
As a result, the selection of metrics and thresholds in TreeNose may not be optimal for detecting selected code smells.
There may be developers not agree with the results of TreeNose, which leads to our second threats to validity. To evaluate the performance of TreeNose,
we conducted an annotation study with 2 developers. Due to the limited number of developers in the study, the results may not be 
generalizable to all developers. We also noted that the amount of subjects and samples are limited in the study. Though we ensured the subjects are actively used and well-maintained, 
the results may still not be generalizable to all software projects. 
To mitigate the threats, we plan to conduct more studies with more developers and subjects in the future.
