\section{Experimental Evaluation of TreeNose}~\label{sec:evaluation}

\vspace*{-1em}

% Integration of content to save space and be direct!
% 1) Describe the context for the empirical study
% 2) Describe the research questions

\noindent
This paper's experiments answer these research questions:

\begin{itemize}[leftmargin=*]

    % RQ1: Manual annotation study
    %
    \item {\it RQ1: How does \texttt{TreeNose} perform in different languages?}
        To answer RQ1, we evaluated \texttt{TreeNose}'s code smell detection
        effectiveness by applying it to 9 open-source projects with at least
        10,000 lines of code written in Java, JavaScript, or Python, as shown in
        the top half of Table~\ref{tab:subject_table}. Adopting a manual
        annotation methodology, we answer RQ1 by calculating precision, recall,
        and the F1 score for both \texttt{TreeNose} and three popular
        language-specific tools.

    % RQ2: Code smell percentages
    %
    \item {\it RQ2: How do code smells distribute in various languages?} To
        answer RQ2, we augmented the 9 projects used to answer RQ1 with 7 more,
        including 4 projects that are multi-language in nature, as shown in
        Table~\ref{tab:subject_table}. Using \texttt{TreeNose}'s output, we
        calculated the percentage of the reported code smells, across the chosen
        projects for each supported language and for multi-language projects.

    % RQ3: Code smell prevalence
    %
    \item {\it RQ3: How often do code smells occur in various languages?}
        Again using the combined subject set in Table~\ref{tab:subject_table},
        we answer RQ3 by calculating a discrepancy score that contextualizes the
        prevalence of a code smell by programming language.

\end{itemize}

% Introduce the subjects (note, now using the term "projects" instead of "subjects")

{\bf Projects}. To empirically answer these three RQs, we applied
\texttt{TreeNose} to real-world open-source projects that met the following
criteria: (i) have commits on main branch in the last year; (ii) have at least
10 thousand lines of code; (iii) have more than 1,000 stars on GitHub; and (iv)
have an implementation entirely in one of Java, JavaScript, or Python or,
alternatively, a multi-language implementation using one or more of the three
aforementioned languages.
%
To answer RQ1, we picked the 9 projects in the top half of
Table~\ref{tab:subject_table}.
%
It is worth noting that, even though \texttt{TreeNose} supports both Python
versions 2 and 3, PySmell only supports Python version 2, thereby necessitating
that all of the Python-based projects must be implemented in version 2 of
Python.
%
In support of the answer to RQ2 and RQ3, the bottom half of
Table~\ref{tab:subject_table} includes one more project in each of the three
chosen languages and four additional multi-language projects.
%
For each project, we performed smell detection on the source code of both the
program and its test suite because the test code is important for a developer
to comprehend and maintain~\cite{ML}.

\input{tables/subject_table}

{\bf Methodology}. To answer RQ1, we compared \texttt{TreeNose} with 3
language-specific code smell detection tools, namely PySmell~\cite{Pysmell} for
Python, JScent~\cite{Jscent} for JavaScript, and
DesigniteJava~\cite{DesigniteJava} for Java. All 3 code smell detectors are
language-specific, AST-based code smell detection tools that are open-source;
PySmell was shown to achieve 97.7\% precision in a prior study.
%
In this paper's experiment, we compared TreeNose to the combination of the
other tools, aiming to mimic the scenario in which developers use multiple code
smell detectors to identify language-specific code smells across projects that
use different languages.
%
Since the aforementioned detectors support most of the code smells detected by
\texttt{TreeNose}, we applied them to each project in the top half of
Table~\ref{tab:subject_table}. To quantify the performance of \texttt{TreeNose}
and the other tools, we adopted precision and recall, which are defined as
follows:

\begin{equation}
    \textit{Precision} = \frac{\textit{True Positive}}{\textit{True Positive} + \textit{False Positive}}
\end{equation}

\begin{equation}
    \textit{Recall} = \frac{\textit{True Positive}}{\textit{True Positive} + \textit{False Negative}}
\end{equation}

Combining precision and recall, we also calculated the F1 score, the harmonic
mean of precision and recall, defined as:

\begin{equation}
    \textnormal{F1} = 2 * \frac{\textit{Precision} * \textit{Recall}}{\textit{Precision} + \textit{Recall}}
\end{equation}

\input{tables/annotation_table}

Since, to our best knowledge, there is no annotated code smell dataset for our
unique combination of projects implemented in both the three languages and in a
multi-language fashion, we designed a manual annotation study.
%
To complete this study we first applied both \texttt{TreeNose} and the other
tools with their default thresholds to analyze the projects in the top half of
Table~\ref{tab:subject_table}.
%
To facilitate a comparison between the presented tool and the language-specific
smell detectors, we first set \texttt{TreeNose} as the ``baseline'' detector
and randomly sampled 5 examples of each code smell.
%
Next, the authors of this paper independently inspected these code smells and
determined whether, from a developer's perspective, they were valid not.
%
The authors then discussed their independent classifications until a consensus
emerged as to whether the detected code smells were valid or
not~\cite{Cruzes2011}.
%
At this step in the manual annotation study, we calculated the precision for
both \texttt{TreeNose} and the language-specific detector and both recall and
the F1 score for the language-specific detector.

% We, furthermore, conducted such a study with 2 internal developers to manually
% verify code smells. Specifically, we built two spreadsheets for the study. One
% for the code smell reports generated by \texttt{TreeNose}, and another for the
% code smell reports generated by the other tools.

% Within the code smell reports
% generated by the other tools, we randomly selected 5 samples out of each code
% smell in each programming language system, and then put those samples in the
% spreadsheet with their related source code and a column for the detection
% decision of \texttt{TreeNose}.



By this approach, we can compare the decisions between the \texttt{TreeNose}
and us, where both can potentially agree or disagree with the other detectors.

We defined the True Positive of \texttt{TreeNose} as the number of code smells
that are detected by both \texttt{TreeNose} and us among the samples selected
from the other tools, the False Positive as the number of code smells that are
detected by \texttt{TreeNose} but not by us, and the False Negative as the
number of code smells that are detected by the developers but not by
\texttt{TreeNose}. With those metrics, we calculated the precision, recall, and
F1 score of \texttt{TreeNose}. Then, we conducted the same study with the
samples from \texttt{TreeNose} to calculate the F1 score of the other tools.
Finally, we utilized the F1 scores of \texttt{TreeNose} and the other tools to
compare their performances. It's noteworthy that not all code smells detected
by \texttt{TreeNose} are detected by the other tools. For those code smells, we
skipped them in the calculation of the F1 score of \texttt{TreeNose} because
there is no detection results from the other tools, and tagged them as not
detected in the calculation of the F1 score of the other tools. We made this
decision because the other tools silently failed to detect those code smells.

% Change 20 to 16 to match up with the Table II Remove "up to" 16 to make it
% precise
%
To answer the RQ2 and RQ3, we implemented \texttt{TreeNose} in a total of 16
open-source projects, including all those listed in
Table~\ref{tab:subject_table}. We analyzed software projects written in 3
selected programming languages, and ones in the
%
% The phrase "the combination of the selected languages" is redundant.
%
combination of them.
%
% Redundant sentence
%We implemented \texttt{TreeNose} in the projects and analyzed the occurrence
%of each code smell in the projects.
%
Analyzing the code smell reports of \texttt{TreeNose} helps to understand the
distribution and prevalence of the selected code smells in different
programming languages, and the target languages' tendencies to contain specific
code smells.

% RQ2, we calculate the number of each code smell in individual projects
% ($s_{k}$) and divide it by the total number of code smells in that project
% ({$S$}) to calculate the project-level percentage of the code smell. Then we
% the sum of the project-level percentages in the projects in the language, and
% divide it by the number of the projects in the language ($N_r$) to calculate
% the percentage of the code smell in the language ($P_{k}$).

% In the experiment of the RQ2, we calculate the percentage of each code smell
% out of all the selected code smells as follows:

% Specify the purpose of Equation 4 at beginning to reinforce the relationship
% between the equation and the RQ2

We calculated the distribution of each code smell ($P_{s}$) with the percentage
formula show in the Equation~\ref{eq:percentage}. To minimize the bias
introduced by the different sizes of the projects in the RQ2, the percentage of
the code smell in was calculated in each project and then averaged in the
projects in the language. We divided the number of each code smell in a project
($m_{k}$) by the number of all kinds of code smells in that project ({$M_{k}$})
to calculate the project-level percentage. Then we took the sum of the
project-level percentages and divided the sum by the number of the projects in
the language ($k$) to calculate $P_{s}$. The results associated with percentages
are shown in Table~\ref{tab:percentage_table}.

% \vspace{-0.5em}

\begin{equation}
P_{s} = \left( \sum_{1}^{k}\frac{{m_{k}}}{M_{k}} \right) \div {k} \times 100\%
\label{eq:percentage}
\end{equation}

% As for the RQ3, we would like to quantify the prevalence of the code smells as
% the occurrence of the code smells in each 1000 lines of code on average
% ($C_{s}$). To calculate it, we divide the number of each code smell in
% individual projects ($s_{k}$) by the total number of lines in the project
% ($N_{l}$) to calculate the project-level prevalence of the code smell, and
% then take the average of the occurrences in each project. Finally by taking the
% average of the occurrences in the projects in the language, we calculate the
% prevalence of the code smell in the language as follows:

As for the RQ3, we quantified the prevalence of a code smell as the average
occurrence of the code smell in each 1000 lines of code ($C_{s}$) shown in the
Equation~\ref{eq:prevalence}. Within this equation, we divided the number of a
code smell in one project ($m_{k}$) by the total number of lines in the project
($N_{k}$) to calculate the project-level occurrence of the code smell, and then
calculate language-level occurrence score ($C_{s}$) by averaging the
project-level occurrences in the projects in the language and multiplying the
result by 1000.

% \vspace{-0.5em}

\begin{equation}
C_{s} = \left( \sum_{1}^{k}{\frac{m_{k}}{N_{k}}} \right) \div k \times 1000
\label{eq:prevalence}
\end{equation}

% With the prevalence score $C_{s}$, we want to quantify how much one language
% outperforms the others. Therefore, we utilized the discrepancy formula to
% calculate the occurrence discrepancy score. We compared the occurrence of one
% language ($C_{s}$) against the same code smell in the combination of the other
% programming languages ($C_{O}$). We calculate the occurrence discrepancy score
% (D) of each code smell in the projects as follows:

With the occurrence score $C_{s}$, we used the discrepancy formula shown in
the Equation~\ref{eq:discrepancy} to quantify performance differences between
the selected programming languages in the RQ3. We calculated the difference with
the occurrence of a code smell in one language ($C_{s}$) minus the same code
smell in the combination of the other languages ($C_{O}$), and calculated the
occurrence discrepancy score (D) by dividing $C_{O}$. The results associated
with discrepancy scores are shown in Table~\ref{tab:occurrence_table}.

% \vspace{-1.0em}

\begin{equation}
    D = (C_{O} - C_{s}) \div C_{O}
    \label{eq:discrepancy}
\end{equation}

In the Equation~\ref{eq:discrepancy}, a positive score indicates that the code
smell occurs less frequently in the language than the other languages, while a
negative score indicates that the code smell occurs more frequently in the
language than the other languages. The discrepancy score has a range of
negative infinity to 1, where 1 means the code smell doesn't occur in the
language at all, and 0 means the code smell occurs in the language as
frequently as the other languages.

{\bf Threats to Validity}. The definition of code smell, code that is hard to
subjective, which is one of the main concerns in this research.
We noted that various code smell detection tools
utilize different metrics and thresholds to detect the same code smell.
Therefore, the metrics of \texttt{TreeNose} may not be consistent with the
other tools. Similarly, the selection of metrics and thresholds in
\texttt{TreeNose} may not be optimal for detecting selected code smells. To
mitigate the threats, we conducted an annotation study with 2 internal authors to
evaluate the performance of \texttt{TreeNose} in detecting code smells.

% Remove the sentence cuz it's redundant with the end of last paragraph
% """Another threat is related to the annotation study. To evaluate the
% performance of \texttt{TreeNose}, we conducted an annotation study with 2
% developers. """
%
Another threat is related to the annotation study. With the limited number of
developers in the study, the results may not be generalizable to all developers.
We also noted that the amount of subjects and samples are limited in the study.
Though we ensured the subjects are actively used and well-maintained, the
results may still not be generalizable to all software projects in the target
language. We plan to conduct more studies with more developers and subjects in
the future.

% Note: in my view, the third threat is not a threat to validity but a
% limitation, so we may delete it?

% The third threat is related to the limitation of the selected code smell
% detectors. They didn't detect all the selected code smells, and the results may
% be biased by the missing code smells. To mitigate the threats, we plan to
% implement more code smell detectors in the future to compare the performance of
% \texttt{TreeNose} with more detectors.

\input{tables/percentage_table}

% Test code may be different than source code of the program

The last threat is due to the decision to include the test code --- along with
the source code of the program under test --- in the analysis used to answer
each research question. While test code is important for developers to
comprehend and maintain in the software development, it may adhere to different
conventions and styles from the program's source
code~\cite{Kapfhammer2004,Kapfhammer2010}. This means that the detection
thresholds of Table~\ref{tab:metrics-and-thresholds}, which are mainly designed
for a program's source code, may not be ideal for the test code. To mitigate
this threat, we plan to conduct future studies that excluding the test code.
