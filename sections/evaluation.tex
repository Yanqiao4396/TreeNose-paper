\input{tables/subject_table}

\section{Experimental Evaluation of TreeNose}~\label{sec:evaluation}

\vspace*{-1em}

To evaluate the performance of \texttt{TreeNose} in different programming
language systems, we implemented \texttt{TreeNose} in 9 large open-source
projects with at least 10,000 lines of code written in Java, JavaScript, or
Python. After building the confidence on the performance of \texttt{TreeNose},
we utilized \texttt{TreeNose} to analyze the structural patterns of the
selected code smells, i.e., distribution and prevalence, in the different
programming languages. To achieve so, we implemented \texttt{TreeNose} in at
least 15 open-source projects, including ones written in the combination of the
selected languages. We designed the following research questions:

\input{tables/annotation_table}

{\it RQ1: How does \texttt{TreeNose} perform in different languages?} This RQ
aims to evaluate the accuracy of \texttt{TreeNose} in detecting code smells in
different programming languages. We selected 3 language-specific code smell
detection tools to compare \texttt{TreeNose}'s detection accuracy with the
combination of theirs.

{\it RQ2: How do code smells distribute in various languages?} This RQ
investigates the distribution of code smells in different programming
languages.

{\it RQ3: How often do code smells occur in various languages?} This RQ intends
to evaluate the prevalence of the selected code smells in different programming
languages.

{\bf Subjects}. To answer the RQs, the subjects must stand for the real-world
software projects in their respective programming languages. They should be
actively used and contain a large amount of code. To achieve this goal, the
subjects in the experiments are 1. have commits on main branch in the last
year, 2. have at least 10 thousands lines of code (KLOC), and 3. have more than
1,000 stars on GitHub. Within those constraints, we selected 3 subjects for
each programming language, Java, JavaScript, and Python, respectively.
Table~\ref{tab:subject_table} shows the characteristics of the subjects. All
the subjects have been proven to have commits on main branch in the last year.
Though TreeNose supports both Python 2 and 3, PySmell only supports Python 2
and therefore all the selected subjects in Python are written in Python 2. And
as for the contents of those projects, we decided to include both source code
and test code in the analysis, it's because the test code is also important for
developer to comprehend and maintain in software development.


{\bf Methodology}. To answer the RQ1, we compared \texttt{TreeNose} with 3
language-specific code smell detection tools, namely PySmell~\cite{Pysmell} for
Python, JScent~\cite{Jscent} for JavaScript, and
DesigniteJava~\cite{DesigniteJava} for Java. All 3 code smell detectors are
language-specific AST-based code smell detection tools that are open-source,
where PySmell was proven to achieve 97.7\% precision in a study. In the
experiment, we compared TreeNose with the combination of the other tools. The
combination of the other tools mimics the scenario where developers utilize
multiple code smell detectors to detect language-specific code smells in
multi-language systems. Those detectors support most code smells detected by
\texttt{TreeNose}, allowing us to implement them in the same sets of projects
with \texttt{TreeNose} to compare their performances. To quantify the
performance of \texttt{TreeNose} and the other tools, we adopted precision and
recall, which are defined as follows:

\vspace{-1.25em}

\begin{equation}
    Precision = \frac{True Positive}{True Positive + False Positive}
\end{equation}

\begin{equation}
    Recall = \frac{True Positive}{True Positive + False Negative}
\end{equation}

Combining precision and recall, we also calculated the F1 score, the harmonic
mean of precision and recall, defined as:

\vspace{-0.5em}

\begin{equation}
    F1 = 2 * \frac{Precision * Recall}{Precision + Recall}
\end{equation}

To our best knowledge, there is no annotated dataset for code smells in our
selected programming languages, we hence had to design an annotation study by
ourselves. First, we implemented both \texttt{TreeNose} and the other tools
with their default thresholds to analyze the same sets of projects. We,
furthermore, conducted such a study with 2 internal developers to manually
verify code smells. Specifically, we built two spreadsheets for the study. One
for the code smell reports generated by \texttt{TreeNose}, and another for the
code smell reports generated by the other tools. Within the code smell reports
generated by the other tools, we randomly selected 5 samples out of each code
smell in each programming language system, and then put those samples in the
spreadsheet with their related source code and a column for the detection
decision of \texttt{TreeNose}. After that, we asked the developers to
independently inspect the samples and determine whether the samples are code
smells or not. Then, we built a consensus from the inspection results, after
the discussion between developers. By this approach, we can compare the
decisions between the \texttt{TreeNose} and the developers, where both can
potentially agree or disagree with the other detectors.

We defined the True Positive of \texttt{TreeNose} as the number of code smells
that are detected by both \texttt{TreeNose} and the developers among the
samples selected from the other tools, the False Positive as the number of code
smells that are detected by \texttt{TreeNose} but not by the developers, and
the False Negative as the number of code smells that are detected by the
developers but not by \texttt{TreeNose}. With those metrics, we calculated the
precision, recall, and F1 score of \texttt{TreeNose}. Then, we conducted the
same study with the samples from \texttt{TreeNose} to calculate the F1 score of
the other tools. Finally, we utilized the F1 scores of \texttt{TreeNose} and
the other tools to compare their performances. It's noteworthy that not all
code smells detected by \texttt{TreeNose} are detected by the other tools. For
those code smells, we skipped them in the calculation of the F1 score of
\texttt{TreeNose} because there is no detection results from the other tools,
and tagged them as not detected in the calculation of the F1 score of the other
tools. We made this decision because the other tools silently failed to detect
those code smells.

%Change 20 to 16 to match up with the Table II
To answer the RQ2 and RQ3, we implemented \texttt{TreeNose} in up to 16
open-source projects, including the subjects in RQ1. We analyzed software
projects written in 3 selected programming languages, and ones in the
% The phrase "the combination of the selected languages" is redundant.
combination of them. 
% Redundant sentence 
%We implemented \texttt{TreeNose} in the projects and analyzed the occurrence of each code
% smell in the projects.
 Analyzing the code smell reports of \texttt{TreeNose}
helps to understand the distribution and prevalence of the selected code smells
in different programming languages, and the target languages'
tendencies to contain specific code smells.

% RQ2, we calculate the number of each code smell in individual projects
% ($s_{k}$) and divide it by the total number of code smells in that project
% ({$S$}) to calculate the project-level percentage of the code smell. Then we
% the sum of the project-level percentages in the projects in the language, and
% divide it by the number of the projects in the language ($N_r$) to calculate
% the percentage of the code smell in the language ($P_{k}$).

% In the experiment of the RQ2, we calculate the percentage of each code smell
% out of all the selected code smells as follows:


% Specify the purpose of Equation 4 at beginning to reinforce the relationship btw the equation and the RQ2
We calculated the distribution of each code smell ($P_{s}$) with the percentage formula show in the Equation~\ref{eq:percentage}.
To minimize the bias introduced by the different sizes of the projects in the
RQ2, the percentage of the code smell in was calculated in each project and then averaged in the projects in the language.
We divided the number of each code smell in a project
($m_{k}$) by the number of all kinds of code smells in that project
({$M_{k}$}) to calculate the project-level percentage. Then we took
the sum of the project-level percentages and
divided the sum by the number of the projects in the language ($k$) to calculate
$P_{s}$. The results associated with percentages are shown in Table~\ref{tab:percentage_table}.

\vspace{-1em}

\begin{equation}
P_{s} = \left( \sum_{1}^{k}\frac{{m_{k}}}{M_{k}} \right) \div {k} \times 100\%
\label{eq:percentage}
\end{equation}

% As for the RQ3, we would like to quantify the prevalence of the code smells as
% the occurrence of the code smells in each 1000 lines of code on average
% ($C_{s}$). To calculate it, we divide the number of each code smell in
% individual projects ($s_{k}$) by the total number of lines in the project
% ($N_{l}$) to calculate the project-level prevalence of the code smell, and
% then take the average of the occurrences in each project. Finally by taking the
% average of the occurrences in the projects in the language, we calculate the
% prevalence of the code smell in the language as follows:


As for the RQ3, we quantified the prevalence of a code smell as
the average occurrence of the code smell in each 1000 lines of code
($C_{s}$) shown in the Equation~\ref{eq:prevalence}.
Within this equation, we divided the number of a code smell in
one project ($m_{k}$) by the total number of lines in the project
($N_{k}$) to calculate the project-level occurrence of the code smell, and
then calculate language-level occurrence score ($C_{s}$) by averaging the project-level occurrences
in the projects in the language and multiplying the result by 1000.

\vspace{-1.0em}

\begin{equation}
C_{s} = \left( \sum_{1}^{k}{\frac{m_{k}}{N_{k}}} \right) \div k \times 1000
\label{eq:prevalence}
\end{equation}

% With the prevalence score $C_{s}$, we want to quantify how much one language
% outperforms the others. Therefore, we utilized the discrepancy formula to
% calculate the occurrence discrepancy score. We compared the occurrence of one
% language ($C_{s}$) against the same code smell in the combination of the other
% programming languages ($C_{O}$). We calculate the occurrence discrepancy score
% (D) of each code smell in the projects as follows:

With the occurrence score $C_{s}$, we utilized the discrepancy formula shown in the Equation~\ref{eq:discrepancy}
to quantify performance differences between the selected programming languages in the RQ3. We calculated the difference with the occurrence of
a code smell in one
language ($C_{s}$) minus the same code smell in the combination of the other languages ($C_{O}$), and calculated the occurrence discrepancy score
(D) by dividing $C_{O}$. The results associated with discrepancy scores are shown in Table~\ref{tab:occurrence_table}.

\vspace{-1.0em}

\begin{equation}
    D = (C_{O} - C_{s}) \div C_{O}
    \label{eq:discrepancy}
\end{equation}

In the Equation~\ref{eq:discrepancy}, a positive score indicates that the code smell occurs less
frequently in the language than the other languages, while a negative score
indicates that the code smell occurs more frequently in the language than the
other languages. The discrepancy score has a range of negative infinity to 1,
where 1 means the code smell doesn't occur in the language at all, and 0 means
the code smell occurs in the language as frequently as the other languages.

{\bf Threats to Validity}. The definition of code smell, code that is hard to
subjective. Therefore, one of concerns in this research is on the definitions
of the selected code smells. We noted that different code smell detection tools
utilize different metrics and thresholds to detect the same code smell.
Therefore, the metrics of \texttt{TreeNose} may not be consistent with the
other tools. As a result, the selection of metrics and thresholds in
\texttt{TreeNose} may not be optimal for detecting selected code smells. To
mitigate the threats, we conducted an annotation study with 2 developers to
evaluate the performance of \texttt{TreeNose} in detecting code smells.

Another treat is related to the annotation study. To evaluate the performance
of \texttt{TreeNose}, we conducted an annotation study with 2 developers. Due
to the limited number of developers in the study, the results may not be
generalizable to all developers. We also noted that the amount of subjects and
samples are limited in the study. Though we ensured the subjects are actively
used and well-maintained, the results may still not be generalizable to all
software projects. To mitigate the threats, we plan to conduct more studies
with more developers and subjects in the future.

The third threat is related to the limitation of the selected code smell
detectors. They didn't detect all the selected code smells, and the results may
be biased by the missing code smells. To mitigate the threats, we plan to
implement more code smell detectors in the future to compare the performance of
\texttt{TreeNose} with more detectors.

\input{tables/percentage_table}

The last threat is due to our decision of including the test code in the
analysis. Test code is important for developers to comprehend and maintain, but
it may have different conventions and styles from the source code. Therefore,
the same thresholds and metrics of TreeNose may not be optimal for the test
code. To mitigate the threats, we plan to conduct more studies with separate
analysis of the source code and test code in the future.
